\documentclass{article}
\usepackage{bnaic}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage[table]{xcolor}
\usepackage{caption}

\newcommand{\eg}{{\it e.g.,}~}
\newcommand{\ie}{{\it i.e.,}~}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}


% ML Jun24: These comments are 
%\section{The bnaic Package}
%The \verb+bnaic.sty+ file is a package that is to be used together with
%the standard \verb+article+ document class. Please adhere strictly to the instructions of this document. The bnaic style file uses the standard \verb+times+ package and the \verb+geometry+ package, which is included in the bnaic package; please do not change it! 

%% if your are not using LaTeX2e use instead
%% \documentstyle[bnaic]{article}

%% begin document with title, author and affiliations


% ML Jun24: These comments are from the BNAIC author instructions

%The title of the article has to appear in bold and the \verb+\huge+ keyword has to be used to set the correct font size. For
%the authors and their affiliations, three cases are distinguished:

%\begin{itemize}
%\item One author: define the author with \verb+\author+ and the affiliation
%   with \verb+\date+.
%\item Multiple authors, all with the same affiliation: define the authors with
%   \verb+\author+, separated by \verb+\and+, and the affiliation with
%   \verb+\date+.
%\item Multiple authors, multiple affiliations: define the
%   authors with \verb+\author+. Put after each name a letter for the
%   affiliation, generated by \verb+\affila+, \verb+\affilb+, etc. On the
%   next lines: one affiliation per line, each preceded by the appropriate letter
%   generated by \verb+\affila+, \verb+\affilb+. See the title of this document.
%\end{itemize}

%Note that affiliations have to appear in italics.

\title{\textbf{\huge Monte Carlo Tree Search for Simultaneous Move Games: A Case Study in the Game of Tron}}
\author{First author \affila \and
    Second author \affila \and
    Third author \affila}
\date{\affila\ \textit{Department of Knowledge Engineering, Maastricht University,\\ P.O. Box 616, 6200 MD Maastricht, The Netherlands}}
%    \affilb\ \textit{The Company Ltd. P.O.Box 4321 Antwerp}}

\pagestyle{empty}

\begin{document}
\ttl
\thispagestyle{empty}


% ML Jun24: From author instructions:
%\section{The abstract}
%Put the abstract before the first section with the \verb+abstract+
%environment. Please start the first paragraph of your abstract with a \verb+\noindent+ command.

\begin{abstract}
\noindent This paper investigates the application of Monte Carlo Tree Search (MCTS) to simultaneous move games. MCTS has been successfully applied to many board games, such as Chess, Checkers and Go. In this paper several variants of MCTS are investigated in order to adapt MCTS to simultaneous move games. Through the experiments, which will be conducted in the test domain in the game of Tron on four different boards, it is shown, that deterministic selection strategies such as sequential UCT, UCB1-Tuned and Decoupled UCT are superior to stochastic selection strategies, such as EXP3, Regret Matching and Decoupled UCT(mix).
\end{abstract}

\section{Introduction}
\label{sec:introduction}
%In the field of Artificial Intelligence (AI), much research in games has been done already. 
An important research topic in Artificial Intelligence (AI) is the development of intelligent agents. 
A classic benchmark is the ability to play games better than human experts. 
These games usually include classic board games, such as Chess~\cite{deep_blue}, Checkers~\cite{schaeffer_2009} and Go~\cite{computer_go} because they are simple to learn yet hard to master.
%In games where a player always has a set of moves to choose from, like the games mentioned above, a good idea is to build a search tree, which evaluates the moves a player can take and eventually chooses one move. 
In classical game tree search, game-specific knowledge is used to determine the strength of each position using a statuc evaluation function~\cite{ai_russel_norvig}. 
If the evaluation function is too complex or a large search tree is required, then either the search has to be increased an alternative approach can be chosen. 

Monte Carlo Tree Search (MCTS)~\cite{chaslot_phd,coulom,kocsis} builds up a search tree without requiring an evaluation function. Instead, it builds the search tree by repeating four phases, as explained below. MCTS was initially applied to the game of Go~\cite{coulom} but has since been applied to many different games and settings~\cite{mctssurvey}. This paper focuses on sampling policies and update rules in MCTS applies to two-player turn-taking simultaneous move games, such as Tron. Some algorithms are investigated in this paper, including sequential UCT~\cite{kocsis} are: UCB1-Tuned, Decoupled UCT, Decoupled UCB1-Tuned EXP3 and Regret Matching.
%This paper deals with MCTS, but not on the aspect of turn-taking board games, but on the application of it to simultaneous move games, such as Tron. 

In Tron, two players move at the same time through a discrete grid and at each move create a wall behind them. 
The first applications of MCTS to Tron, described in~\cite{tron_cig,teuling_tron}, applied standard (sequential) UCT while treating the game as a turn-based alternative move game, in the search tree. A comparison of selection and update policies in simultaneous move MCTS are presented in \cite{cig_paper}. However, results are only presented for a single map and there is no comparison to previous algorithms, such as sequential UCT.
Throughout this paper, we investigated the impact of different selection and update strategies on the playing performance of Monte Carlo Tree Search in the game of Tron.

The paper is organized as follows.
It starts with a brief description of the game Tron and Monte Carlo Tree Search in Subsection~\ref{sec:background}. 
%, followed by an introduction to Monte Carlo Tree Search in Section~\ref{sec:mcts}. 
Section~\ref{sec:tron_specific_mcts} deals with how Monte Carlo Tree Search handles the game specific principles of Tron. In Section~\ref{sec:selection_strategies} the different selection strategies are explained. Afterwards experiments are shown in Section~\ref{sec:experiments} and a conclusion is drawn from the Experiments in Section~\ref{sec:conclusion}. Furthermore, possible future research is also discussed in Section~\ref{sec:conclusion}.

\section{Background: Tron and Monte Carlo Tree Search}

\label{sec:background}
%\label{sec:tron}
%The game of Tron, also known as Light-Cycle Racing, originates from the movie Tron released in 1982. The game is played in a grid like environment, where motorcycles can only drive on the grid, i.e. only drive forward or make 90 degree turns. In addition, the motorcycles create a wall behind them as they drive through the world.\\
%The game which is investigated in this paper,
Tron originates from the 1982 movie with the same name. It is a two-player game (See left half of Figure~\ref{fig:tron-mcts}) played on discrete grids possibly obstructed by walls. In addition, the maps are mostly symmetric so that none of the players have an advantage. Unlike sequential and turn-taking games where payers play consecutively, at each step in Tron both players move simultaneously. The game is won if opponent crashes into a wall or moves off the board. If both players crash at the same turn into a wall, the game ends in a draw.
%To create an intelligent agent for this game is a challenging task, especially for the first moves, many computations have to be done in order to make a profound decision on which move to choose.

\begin{figure}[h]
\begin{center}
\includegraphics[width=4.6cm]{images/tron_field.png} \hspace{0.4cm} \includegraphics[width=9.8cm]{images/mcts_figure.png}
\caption{(Left) A game in Tron. 41 moves are already played. Player 1 started in the top left corner and Player 2 started in the bottom right corner.
 (Right) The four phases of the Monte Carlo Tree Search. \label{fig:tron-mcts}}
\end{center}
\end{figure}


%\section{Monte Carlo Tree Search}
%\label{sec:mcts}
Monte Carlo Tree Search (MCTS)~\cite{coulom, kocsis} is a technique used for decision-making in the turn-based, sequential problems. 
%These decisions are usually move planning decisions in abstract games. 
To make a decision, MCTS makes use of simulations combined with an incrementally-built search tree. 
In the search tree, each node represents a state in the game. To evaluate a state, a game is simulated in self play from the current (root) state of the game until the game is finished. The first part of each simulation will encounter states that are part of the search tree. Which states are encountered in this first part depends on how future states are selected. When MCTS encounters a state that is not in the tree, it is expanded (added to the tree) and then a play-out policy takes over choosing successor states until the end of the game. 
%Play-outs can be done in many different ways, either using a strategy or simply playing random moves. Because one simulation is not a reliable indication whether a move is strong or not, many play-outs are simulated. 
The results of many simulations are back propagated to every node/state until the root node.
MCTS is divided into four phases~\cite{ChaslotWHUB2008}: selection, play-out, expansion and backpropagation. 
%The different phases are explained below and are illustrated in Figure ~\ref{fig:mcts_figure}~\cite{ChaslotWHUB2008}.
These different phases are illustrated in the right half of Figure~\ref{fig:tron-mcts}.

%\begin{figure}[b]
%\begin{center}
%\includegraphics[width=0.9\textwidth]{images/mcts_figure.png}
%\caption{The four phases of the Monte Carlo Tree Search.\label{fig:mcts_figure}}
%\end{center}
%\end{figure}

% Removed the elaborate explanation of each phase for now. 

\section{Tron-Specific MCTS}
\label{sec:tron_specific_mcts}
In the following subsections, it is explained, how MCTS is applied to the game of Tron~\cite{teuling_tron}. In particular how the simultaneous moves of the game are handled and which heuristic knowledge of the game of Tron can be used to increase the performance of the MCTS.

\subsubsection*{Modeling Simultaneous Move Games}

%\label{subsec:sim_moves}
As described in Section~\ref{sec:background}, MCTS applies to sequential turned-based games. However, in Tron actions are chosen simultaneously. Our first model, used to implement Sequential UCT, ignores the presence of simultaneous moves and treat the game as a sequential turn-based game inside the search tree. 
We call this the {\it sequential tree model}. 
In practice, this worked well~\cite{tron_cig,teuling_tron}, except it clearly favors one player which is especially problematic when players are close to each other. Therefore, in our version of Tron we add an exception to the expansion phase. If the first player moves to a position that the second player can also move to, the opponent is still able to move there to achieve a draw rather than a loss~\cite{teuling_tron}. 
In this model, the game is sequential inside the search tree, until a leaf node is reached. The play-outs are then simulated as a simultaneous game~\cite{teuling_tron}.

Our second model, used to implement simultaneous move MCTS, stores a matrix at each node. 
We call this the {\it stacked matrix model}.
Each cell of the matrix corresponds to a joint action, \ie an action chosen by both players simultaneously, and a corresponding child node (successor state). 
This is a more accurate representation of the underlying game since players are not able to base their current decision after having seen the other player's current move choice. 
This is the model used in~\cite{cig_paper,mcts_goofspiel}.

One contribution of this paper is the comparison of MCTS applied in these two different models. 

%\subsection{Heuristic Knowledge}
%\label{subsec:heuristic_knowledge}
%In the game of Tron, the MCTS can make use of some heuristic knowledge of the game to improve the play-out and expansion phase. 

\subsubsection*{Space Estimation, Predictive Expansion Strategy, and Play-out Cut-Offs}

%Space Estimation is useful knowledge to have in the game of Tron. 
Tron is played in a grid-like environment and so often the two players become separated from each other. When this happens, each agent is essentially playing their own single-player game and the goal of the game becomes to outlast the opponent. Therefore the result can be computed by counting the number of squares captured by each player assigning a win to whoever has claimed the most space. The problem is that some positions might not offer a way back and therefore become suicide moves. For that reason a greedy wall-following algorithm can be used, which tries to fill out the remaining space by following a wall. When both players have filled their space, the moves which were made are counted and the player with the higher move count wins. This approach was proposed by Teuling~\cite{teuling_tron}. % and is used in Subsection~\ref{subsec:pes} and~\ref{subsec:play_out_cut_off}.

%\subsubsection*{Predictive Expansion Strategy}
%\label{subsec:pes}
%In the game of Tron, it is common that the players get separated from each other. 
Also, when players are separated, there is no need to let a play-out decide which player would win. Instead, it can be predicted by the Predictive Expansion Strategy (PES)~\cite{teuling_tron}. PES is used to avoid play-outs when they are not necessary. Each time the non-root player tries to expand a node, the PES checks whether the two players are separated from each other. If this is the case, space estimation is used to predict which player would win. Finally, the expanded node becomes a leaf node and no more play-outs have to be done when reaching this node again. %The saved time can be used for more simulations. 

%\subsubsection*{Play-out Cut-Off}
%\label{subsec:play_out_cut_off}

% ML: A lot of this is already said above. 
%During the play-out phase, the game will eventually end and a winner can be determined. Sometimes the outcome of the play-out could be predicted before the play-out is over. %This is useful, because this saves time, which can be used for more simulations~\cite{teuling_tron}. 
%One way to predict the outcome, is to check whether the two players are separated from each other. 
%If this is the case, the space estimation heuristic can be applied again and the winner can be determined. The result will be back propagated in a usual manner. Since computing whether both players are separated from each other takes a lot of time, this is only done every five moves during the play-out.

\section{Selection and Update Strategies}
\label{sec:selection_strategies}

%The default selection strategy is, as mentioned in Subsection~\ref{subsec:selection}, a random strategy, where each child node is selected in a random manner. 
The default selection strategy is to choose a child node uniformly at random.
This can be obviously improved by using heuristics and collected statistics. In the following Subsections, different selection and uopdate strategies are introduced including deterministic strategies such as Sequential UCT, UCB1-Tuned, DUCT(max) and DUCB1-Tuned(max), as well as stochastic strategies, which include DUCT(mix), DUCB1-Tuned(mix), Exp3 and Regret Matching. 

\subsection{Sequential UCT}
\label{subsec:uct}
The most common selection strategy is the Upper Confidence Bounds for Trees (UCT) \cite{kocsis}. The UCT strategy uses the Upper Confidence Bound (UCB1 \cite{auer_et_al}) algorithm. 
%If it is assumed that a sufficient number of play-outs (parameter $T$) have been made for a node
After a sufficient number of play-outs (parameter $T$), UCB1 is used to select a child, otherwise a child is selected randomly. This algorithm maintains a good balance between exploration and exploitation. UCB1 selects a child node $k$ from a set of nodes $K$, from parent node $j$ by using Equation~\ref{uct}:

\begin{equation}\label{uct}
  k = \argmax_{i \in K} \left\{ \bar{X}_{i} + C \sqrt{\frac{\ln (n_{j})}{n_{i}}} \right\},
\end{equation}
where $n_{i}$ is the number of visits of child node $i$ and $\bar{X}_{i}$ is the sample mean of the rewards of child node $i$. 
The parameters $T$ and $C$ are usually tuned to increase performance. 
For the game of Tron these are initially set~\cite{teuling_tron} to $T=30$ and $C=10$.

%\subsection{Sequential UCB1-Tuned}
%\label{subsec:ucb1_tuned}

An enhancement to the UCT selection strategy can be made by replacing the parameter $C$ by a smart upper bound of the variance of the rewards \cite{cig_paper}. This is either $\frac{1}{4}$, which is an upper bound of the variance of a \emph{Bernoulli} random variable, or an upper confidence bound computed using Equation~\ref{ucb1tuned} which has the parameters the parent node $j$ and some child node $i$.
This variant is referred to as UCB1-Tuned~\cite{auer_et_al}.
Then, a child node $k$ is selected from parent node $j$:

\begin{equation}\label{ucb1tuned}
k = \argmax_{i \in K} \left\{ \bar{X}_{i} + \sqrt{\frac{\min(\frac{1}{4},\mbox{Var}_{UCB1}(j,i)) \ln (n_{j})}{n_{i}}} \right\}, 
\mbox{ \hspace{0.1cm} } 
\mbox{Var}_{UCB1}(j,i)=\bar{s} ^2_{k}+\sqrt{\frac{2 \ln (n_{j})}{n_{i}}},
\end{equation}
where $\bar{s} ^2_{k}$ is the sample variance of the observed rewards for child node $k$. 

%\begin{equation}\label{ucb1tuned}
%\end{equation}

\subsection{Decoupled UCT}
\label{subsec:duct}

Unlike standard sequential UCT and UCB1-Tuned apply to the sequential tree model, Decoupled UCT (DUCT) applies UCB1 selection in the stacked matrix model for each player separately.
%In fact, the representation of a node is changed. \
In DUCT a node stores the both moves, the one from player 1 and from player 2. 
UCB1 selects a move twice, once for each player, and independently of the other player's choice.  
To better illustrate the difference between these two concepts, see Figure~\ref{fig:suct_vs_duct}. In the left figure, the sequential tree model is shown. In the right figure figure, each node contains two moves, one belonging to player 1 and one to player 2. These combinations are called {\it joint actions}. Because each level in the search tree represents now one step in the game, the branching factor increases from three to nine.

When selecting a child node, DUCT applies the default UCB1 algorithm which was described in Equation~\ref{uct} with the statistics from each player's perspective independently. After a move is selected player 1, the selection process is repeated for player 2 (without knowledge of the choice made by player 1) using the statistics from player 2's perspective. These two actions are combined to form a joint action.
%Afterwards it combines the result of player 2, for instance the move which represents the move ``right'' and chooses the child node which contains the joint-action [L,R].\\
The final move, after many simulations, can be selected in two different ways. The first, DUCT(max), selects the action, with the most visits. 
DUCT(mix) normalizes the visit counts and samples an action according to this distribution, \ie using a mixed strategy \cite{mcts_goofspiel}. 
%A mixed strategy is constructed such that the probability of each action being played corresponds to the value of the normalized values.
To the best of our knowledge, DUCT(max) and DUCT(mix) where first used in general game-playing programs~\cite{finnson_master,Shafiei09}.

\begin{figure}
\begin{center}
\includegraphics[width=7cm]{images/graph_seq_uct.pdf} \includegraphics[width=7cm]{images/graph_dec_uct.pdf}
\caption{(Left) The sequential tree model. Each node represent the child node of the corresponding move. The first level represents Player 1 moves and the second level represents Player 2 moves. 
L,S and R represents the different moves a player can make. 
(Right) The stacked matrix model. Each node corresponds to a child node from a combination of actions for Player 1 and Player 2. 
\label{fig:suct_vs_duct}}
\end{center}
\end{figure}

%\begin{figure}
%\begin{center}
%\includegraphics[width=8.5cm]{images/graph_dec_uct.pdf}
%\caption{A tree resembling the structure of decoupled MCTS. Each node represents a joint-action consisting of two moves, one for Player 1 and one for Player 2. L,S and R represents the different moves a player can make. L=Left, S=Straight and R=Right. Hence, (L,S) stands for Player1=Left and Player2=Right.\label{fig:graph_duct}}
%\end{center}
%\end{figure}

%\subsection{Decoupled UCB1-Tuned}
%\label{subsec:decoupled_ucb1_tuned}

Just as an enhancement can be made by replacing the parameter $C$ by a smart upper bound of the variance of the rewards in UCT, it can also be made to DUCT. Each time a node is selected and a joint action is chosen, Equations~\ref{ucb1tuned} are used. We refer to this variant as Decoupled UCB1-Tuned.

\subsection{Exp3}
\label{subsec:exp3}

To this point, all selection strategies introduced, except DUCT(mix), are deterministic strategies. DUCT(mix) and Exp3~\cite{Exp3} belong to the group of stochastic selection strategies, which means that there is a random factor involved and instead actions are sampled according to some probability distribution. %that we cannot tell deterministically beforehand which action is going to be chosen. 
Exp3, as DUCT, always uses the stacked matrix model and hence selects joint actions.
Exp3 stores a list of estimated sums of payoffs $\hat{X}_{a^{p}_{k}}$, where $a^{p}_{k}$ refers to player $p$'s action $k$. From the list of payoffs, a policy $P$ is created. The probability of choosing action $a^{p}_{k}$ of policy $P$ is shown in Equation~\ref{exp3_equations}, 

\begin{equation}\label{exp3_equations}
%P_{a^{p}_{k}}=\frac{e^{\hat{X}_{a^{p}_{k}}}}{\sum\limits_{i \in K_{p}}e^{\hat{X}_{a^{p}_{i}}}},
P_{a^{p}_{k}}=\frac{e^{\eta\omega(a^{p}_{k})}}{\sum\limits_{i \in A_{p}}e^{\eta\omega(a^{p}_{i})}}, 
\mbox{\hspace{1cm}}
\hat{X}_{a^{p}_{k}} = \hat{X}_{a^{p}_{k}} + \frac{r^{p}_{a_{k_{1}},a_{k_{2}}}}{\sigma_{a^{p}_{k}}},
\end{equation}
where $K_{p}$ is the set of actions from player $p$, $\omega$ can be scaled by some constant $\eta$, $r^{p}_{a_{k_{1}},a_{k_{2}}}$ is the reward of the play-out when player 1 player chose move $k_{1}$ and player 2 chose move $k_{2}$ and is give in respect to player $p$.
For simplicity, $\eta=1$ is chosen. 
In standard Exp3, $\omega(a^p_k) = \hat{X}_{a^p_k}$, but in practice we use $\omega(a^{p}_{k} ) = \hat{X}_{a^{p}_{k}} - \argmax_{i \in K_{p}}\hat{X}_{a^{p}_{i}}$ since it is equivalent and more numerically stable~\cite{mcts_goofspiel}.
The action selected is then sampled from the mixed strategy where action $a^p_k$ is selected with probability $\sigma_{a^{p}_{k}}=(1-\gamma)  P_{a^{p}_{k}} + \frac{\gamma}{|K_{p}|}$.

%Since Equation~\ref{exp3_policy} leads to numerical instability, Equations~\ref{exp3_policy_new} and~\ref{exp3_policy_weight} are used instead. From this policy, a probability $\sigma_{a^{p}_{k}}$ is computed (See Equation~\ref{exp3_distribution}) from which a mixed strategy is played. 
%Otherwise $\eta$ can be tuned to increase the performance of EXP3.

%\begin{equation}\label{exp3_policy_weight}
%\omega(a^{p}_{k})=\hat{X}_{a^{p}_{k}}-argmax_{i \in A_{p}}\hat{X}_{a^{p}_{i}}
%\end{equation}

%\begin{equation}\label{exp3_distribution}
%\sigma_{a^{p}_{k}}=(1-\gamma) \times P^{new}_{a^{p}_{k}} + \frac{\gamma}{|A_{p}|}
%\end{equation}
%\begin{equation}\label{exp3_update}
%\end{equation}

Parameter $\gamma$ can be optimized by tuning it. The update of $\hat{X}_{p}(a_{n})$ after selecting a joint action $(a_{1},a_{2})$, by using the probability $\sigma_{i}(a_{j})$, which returned some simulation result of a play-out $r^{p}_{a_{k_{1}},a_{k_{2}}}$ is given in Equation~\ref{exp3_equations}.
As in DUCT(mix), the final move is sampled from the normalized visit count distribution.

%The final move is chosen in the same way as it was in DUCT(Mix).


\subsection{Regret Matching}
\label{subsec:rm}
Regret Matching, as EXP3 and DUCT, selects always joint actions. Opposed to the other strategies, Regret Matching stores a matrix $M$ with the estimated mean of the rewards (See Equation~\ref{rm_equations}, where $\bar{X}_{m,n}$ is the mean of the rewards for player 1 when the joint action $(a_{1}=m,a_{2}=n)$ was selected).

\begin{equation}
\label{rm_equations}
M = 
\begin{bmatrix}
\bar{X}_{1,1} && \bar{X}_{2,1} && \bar{X}_{3,1} \\
\bar{X}_{1,2} && \bar{X}_{2,2} && \bar{X}_{3,2} \\
\bar{X}_{1,3} && \bar{X}_{2,3} && \bar{X}_{3,3}
\end{bmatrix}
\mbox{ \hspace{1cm} }
\begin{array}{c}
\forall a^{1}_{i} \in K_{1}, R_{a^{1}_{i}} = R_{a^{1}_{i}} + (\bar{X}_{i,n} - r^{1}_{a_{m},a_{n}})\\
\forall a^{2}_{i} \in K_{2}, R_{a^{2}_{i}} = R_{a^{2}_{i}} + (\bar{X}_{m,i} - r^{2}_{a_{m},a_{n}})
\end{array}
\end{equation}

Additional to matrix $M$, two lists are stores which keep track of the cumulative regret for not taking move $a^p_k$, denoted $R_{a^{p}_{k}}$.
%$is the cumulative regret of player $p$ not having taken action $k$). 
The regret is a value, which indicates how much the player regrets not having played this action. 
%This is the fundamental idea of this selection strategy. 
%With the help of the two lists containing the cumulative regret, 
A policy $P$ is then constructed created by normalizing over the positive cumulative regrets (\eg if the regrets are $R_{a^{1}_{1}} = 8.0$, $R_{a^{1}_{2}} = 5.0$ and $R_{a^{1}_{3}} = -4.0$, then the policy is the probability distribution $(\frac{8}{13}, \frac{5}{13}, 0)$. 
As in Exp3, the selected action is sampled from $\sigma_{a^{p}_{k}}=(1-\gamma) P_{a^{p}_{k}} + \frac{\gamma}{|K_{p}|}$. 
where the variable $\gamma$ can be tuned to increase performance as in Exp3.

%these policies are used to compute a probability from which a mixed strategy is played (See Equation~\ref{regret_eq}. 

%\begin{equation}\label{regret_eq}
%\sigma_{a^{p}_{k}}=(1-\gamma) P_{a^{p}_{k}} + \frac{\gamma}{|A_{p}|}
%\end{equation}

Initially, all values in matrix $M$ and all values in the regret lists are set to zero. After the play-out is finished and the result ($r^{p}_{a_{m},a_{n}}$) of it gets back propagated, the cumulative reward values for each cell are updated using $X_{m,n} = X_{m,n} + r^{p}_{a^{1}_{m},a^{2}_{n}}$ and the regret values are updated using the right side of Equation~\ref{rm_equations}.
The final move is selected as in DUCT(Mix) and EXP3 by using a mixed strategy by normalizing over the visit counts. 

%\begin{equation}\label{update_matrix}
%\begin{align}
%X_{m,n} = X_{m,n} + r^{p}_{a^{1}_{m},a^{2}_{n}}
%\end{align}
%\end{equation}

%\begin{equation}\label{update_regret}
%\begin{align}
%\end{align}
%\end{equation}


\section{Experiments}
\label{sec:experiments}
In this section the different selection strategies, which were proposed in Section~\ref{sec:selection_strategies} are tested. Because the framework, which was implemented by Teuling~\cite{teuling_tron}, used for this experiments is based on sequential UCT, DUCT, EXP3 and Regret Matching were not optimally implemented. Therefore it would be an unfair comparison, if all agents would have the same thinking time. In order to make it a fair comparison, each agent is allowed to simulate a fixed number of simulations. In the case of this experiment the number is set to 100,000. The experiments are run on four different boards (Three boards with obstacles on it (See Fig.~\ref{fig:ex_boards}) and an empty board (d)), all with dimensions of 16$\times$16. On each board 200 games are played and, even though all the boards are symmetric, the starting positions are switched after half of the games, to assure that none of the agents are benefiting from starting in a certain position. The play-out strategy, which is used in all experiments is the random strategy with play-out cut-off as an enhancement. Also, the expansion strategy uses the predictive expansion strategy.\\
Some parameters are tuned in Subsection~\ref{subsec:parameter_tuning} and the several selection strategies introduced in this paper, are tested against each other in a round-robin tournament in Subsection~\ref{subsec:round_robin}. In addition another round-robin tournament is conducted on a smaller 6$\times$6 board to see how the results differ from the ones conducted in the round-robin tournament on the large boards.

\begin{figure}[t]
\begin{center}
\includegraphics[width=8.5cm]{images/boards.png}
\caption{Four different boards are used for the experiments in the round-robin tournament. a,b and c, as well as the empty board, d, of the same size.\label{fig:ex_boards}}
\end{center}
\end{figure}

\subsection{Parameter Tuning}
\label{subsec:parameter_tuning}
In the beginning, some parameters ($C$ and $T$ in UCT and $\gamma$ in EXP3 and Regret Matching) are tuned. This is done, to assure that the different strategies perform optimally. As reference constants, values are used, which were taken from different sources~\cite{teuling_tron,cig_paper}. Parameter $C$, which is used in UCB1 (See Equation~\ref{uct}) was tuned, by letting a UCT player playing games against a MCTS player with a random selection and a random play-out strategy. After 10 games the value of the parameter $C$ was slightly increased or decreased, depending on how strong the UCT player played. Starting with the reference constant, 120 games were played in order to find a value for $C$ which did not change significantly anymore.\\
The parameter $T$ used in UCT and UCB1-Tuned, which is the threshold before either UCT or UCB1-Tuned is applied as a selection strategy was tuned in the same way as the parameter $C$. This parameter tuning showed the same result as shown by Teuling~\cite{teuling_tron}, namely that the parameter $T$ does not influence the performance of UCT and UCB1-Tuned.\\
The parameter $\gamma$ of EXP3 and Regret Matching was also tuned in the same way as the parameter $C$ with the only difference, that $\gamma$ must be in the interval of $[0,1]$.\\
All the tuned values can be seen in Table~\ref{table:parameter_tuning}.

\begin{table}[b]\footnotesize
\caption{Results of the parameter-tuning of parameters $C$ and $T$ from the UCT Algorithm and from the parameter $\gamma$ of the EXP3 algorithm}
\centering
\begin{tabular}{|c||c|c|}
							\hline
Parameter			& Reference constant		& Tuned value	\\ \hline\hline
$C$				& 10~\cite{teuling_tron}	& 13.2		\\ \hline
$T$				& 30~\cite{teuling_tron}	& 30		\\ \hline
$\gamma$(EXP3)			& 0.36~\cite{cig_paper}		& 0.39		\\ \hline
$\gamma$(Regret Matching)	& 0.36				& 0.31		\\ \hline

\end{tabular}
\label{table:parameter_tuning}
\end{table}

\subsection{Round-Robin Tournament}
\label{subsec:round_robin}
In this subsection, several players using different selection strategies are tested. In Table~\ref{table:round_robin} the results of the games on all four maps are seen and Table~\ref{table:round_robin_total} shows the average performance of all players.

\begin{table*}\scriptsize
\caption{Results of the different selection strategies playing on Board a,b,c and d.}
\centering
\begin{tabular}{|c||c|c|c|c|c|c|c|c|}
									\hline
  Board a 		& UCT 	& UCB1T		& DUCT(Max)	& DUCT(Mix)	& DUCB1T(Max)	& DUCB1T(Mix)	& EXP3	& RM				\\ 	\hline\hline
  UCT 			&  -	& 61\%		& 51\% 		& 54\%		& 53\%		& 67\%		& 65\%	& 62\%				\\ 	\hline
  UCB1T 		& 39\% 	& - 		& 53\%		& 61\%		& 52\%		& 65\%		& 66\%	& 61\%				\\ 	\hline
  DUCT(Max) 		& 49\% 	& 47\%		& -		& 67\%		& 43\%		& 62\%		& 63\%	& 67\%				\\ 	\hline
  DUCT(Mix) 		& 46\% 	& 39\% 		& 33\%		& -		& 37\%		& 48\%		& 53\%	& 51\%				\\ 	\hline
  DUCB1T(Max)	 	& 47\% 	& 48\% 		& 57\%		& 63\%		& -		& 61\%		& 64\%	& 69\%				\\ 	\hline
  DUCB1T(Mix)	 	& 33\% 	& 35\% 		& 38\%		& 52\%		& 39\%		& -		& 54\%	& 51\%				\\ 	\hline
  EXP3 			& 35\% 	& 34\% 		& 37\%		& 47\%		& 36\%		& 46\%		& -	& 51\%				\\ 	\hline
  RM		 	& 38\%	& 39\% 		& 33\%		& 49\%		& 31\%		& 49\%		& 49\%	& -				\\ 	\hline
\end{tabular}

\begin{tabular}{|c||c|c|c|c|c|c|c|c|}
									\hline
  Board b 		& UCT 	& UCB1T		& DUCT(Max)	& DUCT(Mix)	& DUCB1T(Max)	& DUCB1T(Mix)	& EXP3	& RM				\\ 	\hline\hline
  UCT 			&  -	& 47\%		& 49\%		& 57\%		& 51\%		& 61\%		& 61\%	& 59\%				\\ 	\hline
  UCB1T 		& 53\% 	& - 		& 51\%		& 54\%		& 49\%		& 65\%		& 64\%	& 57\%				\\ 	\hline
  DUCT(Max) 		& 51\% 	& 49\% 		& -		& 54\%		& 51\%		& 58\%		& 54\%	& 52\%				\\ 	\hline
  DUCT(Mix) 		& 43\%	& 46\% 		& 46\%		& -		& 41\%		& 52\%		& 49\%	& 46\%				\\ 	\hline
  DUCB1T(Max)	 	& 49\% 	& 51\% 		& 49\%		& 59\%		& -		& 68\%		& 62\%	& 65\%				\\ 	\hline
  DUCB1T(Mix)	 	& 39\% 	& 35\% 		& 42\%		& 48\%		& 32\%		& -		& 57\%	& 52\%				\\ 	\hline
  EXP3 			& 39\% 	& 36\% 		& 46\%		& 51\%		& 38\%		& 43\%		& -	& 57\%				\\ 	\hline
  RM 			& 41\% 	& 43\% 		& 48\%		& 54\%		& 35\%		& 48\%		& 43\%	& -				\\ 	\hline
\end{tabular}

\begin{tabular}{|c||c|c|c|c|c|c|c|c|}
									\hline
  Board c 		& UCT 	& UCB1T		& DUCT(Max)	& DUCT(Mix)	& DUCB1T(Max)	& DUCB1T(Mix)	& EXP3	& RM				\\ 	\hline\hline
  UCT 			&  -	& 37\% 		& 81\%		& 89\%		& 84\%		& 96\%		& 71\%	& 72\%				\\ 	\hline
  UCB1T 		& 63\% 	& - 		& 79\%		& 71\%		& 88\%		& 89\%		& 73\%	& 70\%				\\ 	\hline
  DUCT(Max) 		& 19\% 	& 21\% 		& -		& 58\%		& 41\%		& 52\%		& 69\%	& 61\%				\\ 	\hline
  DUCT(Mix) 		& 11\% 	& 29\% 		& 42\%		& -		& 39\%		& 49\%		& 51\%	& 51\%				\\ 	\hline
  DUCB1T(Max)	 	& 16\% 	& 12\% 		& 59\%		& 61\%		& -		& 59\%		& 68\%	& 64\%				\\ 	\hline
  DUCB1T(Mix)	 	& 4\%  	& 11\% 		& 48\%		& 51\%		& 41\%		& -		& 50\%	& 58\%				\\ 	\hline
  EXP3 			& 29\% 	& 27\% 		& 31\%		& 49\%		& 32\%		& 50\%		& -	& 49\%				\\ 	\hline
  RM 			& 28\% 	& 30\% 		& 39\%		& 49\%		& 36\%		& 42\%		& 51\%	& -				\\ 	\hline
\end{tabular}

\begin{tabular}{|c||c|c|c|c|c|c|c|c|}
									\hline
  Board d 		& UCT 	& UCB1T		& DUCT(Max)	& DUCT(Mix)	& DUCB1T(Max)	& DUCB1T(Mix)	& EXP3	& RM				\\ 	\hline\hline
  UCT 			&  -	& 52\%		& 48\%		& 51\%		& 50\%		& 64\%		& 67\%	& 65\%				\\ 	\hline
  UCB1T 		& 48\% 	& - 		& 45\%		& 57\%		& 53\%		& 67\%		& 59\%	& 67\%				\\ 	\hline
  DUCT(Max) 		& 52\% 	& 55\% 		& -		& 60\%		& 49\%		& 61\%		& 59\%	& 62\%				\\ 	\hline
  DUCT(Mix) 		& 49\% 	& 43\% 		& 40\%		& -		& 41\%		& 55\%		& 50\%	& 49\%				\\ 	\hline
  DUCB1T(Max)	 	& 50\% 	& 47\% 		& 51\%		& 59\%		& -		& 57\%		& 64\%	& 68\%				\\ 	\hline
  DUCB1T(Mix)	 	& 36\% 	& 33\% 		& 39\%		& 45\%		& 43\%		& -		& 51\%	& 49\%				\\ 	\hline
  EXP3 			& 33\% 	& 41\% 		& 41\%		& 50\%		& 36\%		& 49\%		& -	& 52\%				\\ 	\hline
  RM 			& 35\% 	& 23\% 		& 38\%		& 51\%		& 32\%		& 51\%		& 48\%	& -				\\ 	\hline
\end{tabular}
\caption*{UCB1T=UCB1-Tuned, DUCB1T(Max)=DUCB1-Tuned(Max), DUCB1T(Mix)=DUCB1-Tuned(Mix), RM=Regret Matching}
\label{table:round_robin}
\end{table*}

\begin{table*}\scriptsize
\caption{Average results of the different selection strategies playing against each other.}
\centering
\begin{tabular}{|c||c|c|c|c|c|c|c|c|}
									\hline
  Total 		& UCT 	& UCB1T		& DUCT(Max)	& DUCT(Mix)	& DUCB1T(Max)	& DUCB1T(Mix)	& EXP3	& RM				\\ 	\hline\hline
  UCT 			&  -	& 49\%		& 57\%		& 63\%		& 60\%		& 72\%		& 66\%	& 64\%				\\ 	\hline
  UCB1T 		& 51\% 	& - 		& 57\%		& 61\%		& 61\%		& 71\%		& 65\%	& 63\%				\\ 	\hline
  DUCT(Max) 		& 43\% 	& 43\% 		& -		& 60\%		& 46\%		& 58\%		& 61\%	& 60\%				\\ 	\hline
  DUCT(Mix) 		& 37\% 	& 39\% 		& 40\%		& -		& 40\%		& 51\%		& 51\%	& 49\%				\\ 	\hline
  DUCB1T(Max)	 	& 40\% 	& 39\% 		& 54\%		& 60\%		& -		& 61\%		& 65\%	& 67\%				\\ 	\hline
  DUCB1T(Mix)	 	& 18\% 	& 19\% 		& 42\%		& 49\%		& 39\%		& -		& 53\%	& 52\%				\\ 	\hline
  EXP3 			& 34\%	& 35\% 		& 39\%		& 49\%		& 35\%		& 47\%		& -	& 52\%				\\ 	\hline
  RM		 	& 36\% 	& 37\%		& 40\%		& 51\%		& 33\%		& 48\%		& 48\%	& -				\\ 	\hline
\end{tabular}
\caption*{UCB1T=UCB1-Tuned, DUCB1T(Max)=DUCB1-Tuned(Max), DUCB1T(Mix)=DUCB1-Tuned(Mix), RM=Regret Matching}
\label{table:round_robin_total}
\end{table*}

From Table~\ref{table:round_robin_results} we can see, that the sequential UCT and UCB1-Tuned selection strategies perform best. The Max versions of the decoupled UCT and UCB1-Tuned strategy still performs reasonably well ($>50\%$). All the other strategies, which include the Mix versions of the decoupled UCT and UCB1-Tuned, as well as EXP3 and Regret Matching, perform not that well ($<50\%$).\\
Furthermore it stands out that on Board c, the sequential MCTS out performs  the decoupled versions of MCTS.

The same selection strategies were tested again against each other in another round-robin tournament, which will be played on a 6$\times$6 board. From the results in Table~\ref{table:rr_small} can be seen, that deterministic selection strategies are still doing better that stochastic ones. But, stochastic strategies perform better on the 6$\times$6 board than on the larger 16$\times$16 board.

\subsection{Discussion}
\label{subsec:discussion}

The experiments revealed that the parameter $C$ of the UCT selection strategy was optimal at 13.2. Usually, the parameter $C$ is fairly low, but in Tron, there are only 3 possible moves, therefore it is beneficial to know where all three moves would lead us. A high $C$ parameter ensures that the exploration of all nodes is given, which explains the result of the high $C$ parameter.\\
The parameter $T$ of the UCT and UCB1-Tuned selection strategy was tuned. After some experiments, the tuning was stopped, because it did not seem to have much impact on the result of the selection strategies. Therefore, 30 is still used for the parameter $T$. The parameter $\gamma$ from the EXP3 and Regret Matching selection strategy was tuned to 0.39 and 0.31, respectively, to increase performance of those strategies.\\
The round-robin tournament, in which every selection strategy played against each other, showed that the sequential UCB1-Tuned algorithm wins on average 63\% of its games and is therefore the winner of the tournament. Further it may be concluded, that the sequential strategies perform better than the decoupled strategies, which is especially visible on Board c (See Table~\ref{table:round_robin}), where the UCT and UCB1-Tuned only lost 4\%-19\% of its games against all DUCT strategies and only 28\%-29\% against EXP3 and Regret Matching, respectively. Additionally, it can be concluded that deterministic selection strategies perform superior than stochastic selection strategies on 16$\times$16 boards (All deterministic strategies win $>50\%$ and all stochastic win $<50\%$ of their matches). Already the selection strategies, which are deterministic, but only use a stochastic process to determine their final move, such as DUCT(Mix) and DUCB1-Tuned(Mix), perform way weaker than their counterparts DUCT(Max) and DUCB1-Tuned(Max).\\
Furthermore, the round-robin tournament on the smaller 6$\times$6 board showed, that if a smaller board is used, the performance of the deterministic strategies decrease and the performance of the stochastic strategies increase. This indicates, that as soon as both players come closer together, using a stochastic strategy might be more suitable than a deterministic one. This phenomenon is illustrated in Figure~\ref{fig:stochastic}, where both players have two possible moves. If both players choose $a^{p}_{1}$ (``Left''), Player 1 wins and if both players choose $a^{p}_{2}$ (``Right''), Player 1 also wins. Therefore it might be beneficial for Player 2 to play with a mixed strategy, where the probabilities for choosing actionfor $a^{2}_{1}=0.5$ and for $a^{2}_{2}=0.5$.

\begin{figure}
\begin{center}
\includegraphics[width=8.5cm]{images/stochastic.pdf}
\caption{A Position, which indicates that mixing strategies is important.}
\label{fig:stochastic}
\end{center}
\end{figure}

\section{Conclusion and Future Research}
\label{sec:conclusion}

\begin{table}[h]\scriptsize
\caption{Results of the different selection strategies playing against each other. a,b,c and d stand for the four different boards, which are used in the round-robin tournament. $\pm$ refers to 95\% confidence intervals.}
\centering
\begin{tabular}{|c||c|c|c|c|c|}
									\hline
	Selection Strategy	& a 		  & b 		  & c 		  & d 		  & Total 	\\ \hline
	UCB1-Tuned		      & 57\%	  & 58\%		& 78\%		& 58\%		& 63$\pm$1.26\%		\\ \hline
	UCT			            & 59\%		& 55\%		& 76\%		& 57\%		& 61$\pm$1.28\%		\\ \hline
	DUCB1-Tuned(Max)	  & 58\%		& 58\%		& 48\%		& 57\%		& 55$\pm$1.30\%		\\ \hline
	DUCT(Max)		        & 56\%		& 53\%		& 37\%		& 57\%		& 51$\pm$1.31\%		\\ \hline
	DUCT(Mix)		        & 44\%		& 46\%		& 39\%		& 47\%		& 45$\pm$1.30\%		\\ \hline
	DUCB1-Tuned(Mix)	  & 43\%		& 44\%		& 38\%		& 42\%		& 42$\pm$1.29\%		\\ \hline
	EXP3			          & 41\%		& 44\%		& 38\%		& 43\%		& 42$\pm$1.29\%		\\ \hline
	Regret Matching		  & 41\%		& 44\%		& 39\%		& 40\%		& 41$\pm$1.28\%		\\ \hline
\end{tabular}
\label{table:round_robin_results}
\end{table}

\begin{table*}\scriptsize
\caption{Results of the different selection strategies playing against each other on a 6$\times$6 board. $\pm$ refers to 95\% confidence intervals.}
\centering
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|}
									\hline
	6$\times$6 Board	& UCB1T 	& UCT		& DUCT(Max)	& DUCB1T(Max)	& DUCT(Mix)	& DUCB1T(Mix)	& RM	& EXP3	& Total			\\ \hline
	UCB1T							& -				& 52\%	& 59\%			& 62\%				& 59\%			& 62\%				& 57\%& 61\%	& 59$\pm$2.57\%		\\ \hline
	UCT								& 48\%		& -			& 55\%			& 61\%				& 57\%			& 56\%				& 60\%& 59\%	& 57$\pm$2.56\%		\\ \hline
	DUCT(Max)					& 41\%		& 45\%	& -					& 48\%				& 56\%			& 56\%				& 54\%& 60\%	& 52$\pm$2.62\%		\\ \hline
	DUCB1T(Max)				& 38\%		& 39\%	& 52\%			& -						& 56\%			& 57\%				& 63\%& 61\%	& 52$\pm$2.62\%		\\ \hline
	DUCT(Mix)					& 41\%		& 43\%	& 44\%			& 44\%				& -					& 53\%				& 50\%& 49\%	& 47$\pm$2.61\%		\\ \hline
	DUCB1T(Mix)				& 38\%		& 44\%	& 44\%			& 43\%				& 43\%			& -						& 51\%& 54\%	& 46$\pm$2.61\%		\\ \hline
	RM								& 43\%		& 40\%	& 46\%			& 37\%				& 50\%			& 46\%				& -		& 46\%	& 46$\pm$2.61\%		\\ \hline
	EXP3							& 39\%		& 41\%	& 40\%			& 39\%				& 51\%			& 41\%				& 54\%& -			& 44$\pm$2.60\%		\\ \hline
\end{tabular}
\caption*{UCB1T=UCB1-Tuned, DUCB1T(Max)=DUCB1-Tuned(Max), DUCB1T(Mix)=DUCB1-Tuned(Mix), RM=Regret Matching}
\label{table:rr_small}
\end{table*}

In this paper, several selection strategies for the selection phase were introduced, including the deterministic strategies UCT, UCB1-Tuned, DUCT(Max) and DUCB1-Tuned(Max) and the stochastic strategies DUCT(Mix), DUCB1-Tuned(Mix), EXP3 and Regret Matching. All these enhancements were tested in the game of Tron on different boards to see which strategies perform better than others.\\
Overall, the round-robin tournament showed, that UCB1-Tuned performs the best in the game of Tron. Furthermore the experiments showed, that deterministic strategies are superior to stochastic ones, but also that the performance of stochastic strategies increases as the board gets smaller. It also showed, that the layout of the board has great influences on the outcome of a game.\\
For future research, more experiments with different boards are required, in order to determine why some boards (as Board c) create more difficulties for the decoupled strategies.\\
In addition, the selection strategy EXP3 can be enhanced by tuning parameter $\eta$, which was set to $\eta=1$ for simplicity.\\
Moreover, a hybrid selection strategy should be tested, which uses a deterministic strategy if both players are far away from each other and a stochastic one as soon as both players come fairly close to each other. This hybrid selection strategy would indicate, if the assumption made in this paper, that stochastic strategies perform better when both players are close to each other, is correct.



\bibliographystyle{plain}
\bibliography{sm-tron}



\end{document}








